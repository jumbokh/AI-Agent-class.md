```
çœŸçš„å¼·åˆ°è®“æˆ‘ä¸‹å·´éƒ½æ‰ä¸‹ä¾†ï¼æˆ‘å• GPT-OSS:20B æ¨¡å‹
OpenAI é–‹æ”¾åŸå§‹ç¢¼çš„é€™å…©å€‹æ¨¡å‹ï¼ŒçœŸçš„æ‰“éå¤©ä¸‹ï¼
ä¸€å®šè¦é¦¬ä¸Šå®‰è£èµ·ä¾†è©¦è©¦çœ‹ï¼
æˆ‘å¯ä»¥å¾®èª¿ GPT-OSS:20B æ¨¡å‹å—ï¼Ÿä¾‹å¦‚æˆ‘æƒ³è®“ GPT-OSS:20B èƒ½å¤ å°‡ è¯èª ç¿»è­¯æˆ å°èª ï¼Œå°èª ç¿»è­¯æˆ è¯èª
ä»–åªæƒ³äº† 2.7ç§’ å›æ‡‰å¦‚ä¸‹ï¼š
âœ… GPTâ€‘OSSâ€‘20B å¾®èª¿ï¼ˆè¯èª â†” å°èªç¿»è­¯ï¼‰
ä¸€å¥è©±
ä½ å¯ä»¥æŠŠ GPTâ€‘OSSâ€‘20B åšã€ŒLoRA / QLoRAã€å¾®èª¿ï¼Œè®“å®ƒå°ˆé–€åœ¨ã€Œè¯èª â†” å°èªã€ä¸Šåšç¿»è­¯ã€‚
åªéœ€è¦ä¸€å¼µ 24 GB GPUï¼ˆRTX 4090 / RTX 4060Ti+ï¼‰æˆ–å¤šå¼µ 80 GB+ GPU å°±èƒ½è·‘ã€‚
å®Œæˆå¾Œå†è½‰æˆ .ggufï¼Œå°±èƒ½åœ¨æœ¬åœ°ï¼ˆOllamaï¼‰ç›´æ¥ä½¿ç”¨ã€‚
```
```
1ï¸âƒ£ å…ˆèªªèªªã€Œå¯è¡Œæ€§ã€èˆ‡ã€Œæˆæœ¬ã€
è³‡æº æˆæœ¬ å¤±æ•—é¢¨éšª
RTX 4060 (8 GB) â˜…â˜… 20 B 24â€‘bit â†’ 30 GB ä»¥ä¸Šï¼›ç„¡æ³•ç›´æ¥è¨“ç·´ã€‚å¯ç”¨ QLoRAï¼ˆ4â€‘bitï¼‰åœ¨åŒä¸€å¡ä¸Šè·‘ï¼ˆâ‰ˆ 30 GB åƒæ•¸ + 8 GB RAMï¼‰ã€‚
RTX 4090 (24 GB) â˜…â˜…â˜… 20 B 4â€‘bit è¨“ç·´ï¼ˆâ‰ˆ 48 GBï¼‰éœ€è¦ å¤šå¡ (â‰¤ 2â€‘4 å¡) + NVLinkã€‚å¯ç”¨ accelerate çš„ fsdp æˆ– ddpã€‚
A100 40 GB Ã— 2 (80 GB) â˜…â˜…â˜…â˜… 20 B 4â€‘bit (â‰ˆ50 GB) å¾ˆå®¹æ˜“è·‘ã€‚å»ºè­°ä½¿ç”¨ Cloud (AWS, GCP, Azure) æˆ–è‡ªå®¶ GPU å¢é›†ã€‚
å¯¦éš›å¯è¡Œè·Ÿä½ å° GPU è¨˜æ†¶é«”/è¨ˆç®—èƒ½åŠ›ç›´æ¥æ›éˆã€‚
```
2ï¸âƒ£ é–‹ç™¼ç’°å¢ƒï¼ˆæœ€ä½éœ€æ±‚ï¼‰
```
# 1ï¸âƒ£ å»ºç«‹è™›æ“¬ç’°å¢ƒï¼ˆPython 3.11+ï¼‰
python -m venv venv
source venv/bin/activate         # Windows: venv\Scripts\activate
```
# 2ï¸âƒ£ å®‰è£ PyTorch + CUDA 12.x
```
pip install torch==2.4.0+cu124 torchaudio --index-url https://download.pytorch.org/whl/cu124
```
# 3ï¸âƒ£ å®‰è£å…¶ä»–åº«
```
pip install transformers==4.45.2 datasets peft==0.13.0 trl accelerate bitsandbytes
```
# 4ï¸âƒ£ (å¯é¸) å®‰è£ Hf Hub çš„ CLI æ–¹ä¾¿ä¸‹è¼‰æ¨¡å‹
```
pip install huggingface-hub
```
é—œéµ
```
bitsandbytes å…è¨± 4â€‘bit/8â€‘bit ç²¾åº¦ï¼›
transformers æœƒè‡ªå‹•è®€ GPTâ€‘OSSâ€‘20B çš„ tokenizerã€‚
trl æ–¹ä¾¿åš SFTï¼ˆInstructionâ€‘Fineâ€‘Tuningï¼‰ã€‚
```
<pre>
3ï¸âƒ£ æº–å‚™ç¿»è­¯æ•¸æ“š
3.1 æ•¸æ“šæ ¼å¼
æœ€ç°¡å–®çš„æ˜¯ JSON Lines æˆ– TSVã€‚ä¸‹é¢æ˜¯ä¸€å€‹ç¤ºä¾‹ï¼ˆzh_taich.tsvï¼‰ï¼š
ç¹é«”ä¸­æ–‡åŸå¥\tå°èªè­¯å¥
ä½ å¥½ï¼Œè«‹å•ä½ å¥½å—ï¼Ÿ\tä½ å¥½å’§ï¼Œè«‹å•æå¥½ç„¡ï¼Ÿ
ä»Šæ—¥å¤©æ°£æ™´æœ—ï¼Œé©åˆå»éƒŠå¤–èµ°èµ°ã€‚\tä»Šä»”æ—¥å¤©æ°£æ™´æœ—ï¼Œé©åˆå»éƒŠå¤–èµ°æ­¥ã€‚
ä½ å¯ä»¥è‡ªè¡Œè’é›†ï¼Œä¾‹å¦‚ï¼š
é–‹æºèªæ–™åº«ï¼ˆå¦‚ zhâ€‘tc â†’ zhâ€‘tw ç¿»è­¯å°ï¼‰ã€‚
ä½ è‡ªå·±çš„å°ç…§å¥å­ã€‚
ä»»ä½•èƒ½å¤ ç”¢ç”Ÿ source / target pair çš„æ ¼å¼ã€‚
3.2 æŠŠæˆ Hugging Face Dataset
# load_and_save.py
from datasets import load_dataset, DatasetDict
# å¦‚æœæ˜¯ TSV
ds = load_dataset("csv", data_files="zh_taich.tsv", delimiter="\t")
ds = ds.rename_column("column_0", "src").rename_column("column_1", "tgt")
ds = ds.remove_columns("id")   # è‹¥æœ‰ id è¡Œ
# åˆ‡æˆ train/dev
split = ds.train_test_split(test_size=0.05, seed=42)
split["train"].save_to_disk("./data/zh_taich")  # æ–¹ä¾¿é‡è¼‰
åŸ·è¡Œä¸€æ¬¡å¾Œï¼Œ./data/zh_taich æœƒåŒ…å« train.arrowã€test.arrow ç­‰ã€‚
4ï¸âƒ£ æ–‡å­—åˆ†è©èˆ‡ Tokenizer
ä½¿ç”¨ GPTâ€‘OSSâ€‘20B çš„ tokenizerï¼›å¦‚æœä½ ç”¨çš„æ˜¯ gguf æ ¼å¼ï¼Œè«‹å…ˆæŠŠæ¨¡å‹è½‰æˆ HuggingFace æ¨¡å‹æˆ–ä¸‹è¼‰ HF ç‰ˆæœ¬ï¼ˆä¾‹å¦‚ zhipuai/gpt-oss-20bï¼‰ã€‚
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("zhipuai/gpt-oss-20b")  # é€™æ˜¯å°æ‡‰çš„ HF repoï¼Œè«‹è‡ªè¡Œç¢ºèª
tokenizer.model_max_length = 4096   # æˆ–ä½ é æœŸçš„æœ€å¤§åºåˆ—é•·
Tip: è‹¥ä½ å·²ç¶“åœ¨æœ¬æ©Ÿåªæ“æœ‰ ggufï¼Œå¯ä»¥å…ˆè·‘ transformers2gguf æŠŠå®ƒè½‰å› HFï¼Œæˆ–ç›´æ¥ç”¨ transformers è®€ .bin éƒ¨åˆ†ï¼ˆä¸æ”¯æ´ .ggufï¼‰ã€‚
ç›®å‰å¤§å¤šæ•¸å¾®èª¿æµç¨‹éœ€è¦ tf, pt ç‰ˆæœ¬çš„ weightã€‚
5ï¸âƒ£ LoRA / QLoRA å¾®èª¿è…³æœ¬
ä¸‹é¢çš„ç¯„ä¾‹ä½¿ç”¨ QLoRA + PEFTï¼Œä¿ç•™åŸºç¤æ¬Šé‡ä¸è®Šï¼Œåªå¾®èª¿ 4â€‘bit å¢é‡å±¤ï¼Œè¨˜æ†¶é«”éœ€æ±‚å¤§å¹…ä¸‹é™ã€‚
ä½ å¯ä»¥æ ¹æ“šè‡ªå·±çš„ GPU é…ç½®èª¿æ•´ lora_rã€lora_alphaã€quantization_bit ç­‰åƒæ•¸ã€‚
</pre>
```
# train_zh_taich.sh
#!/usr/bin/env bash
source venv/bin/activate      # é€²å…¥è™›æ“¬ç’°å¢ƒ
accelerate launch train.py
```
```
# train.py
import os, torch, datasets, evaluate, json
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training
import bitsandbytes as bnb
# ==== 1ï¸âƒ£ é…ç½® ====================================================
MODEL_NAME = "zhipuai/gpt-oss-20b"          # HF repo æˆ–ä½ è‡ªå·±çš„ local è·¯å¾‘
DATA_PATH = "./data/zh_taich"
OUTPUT_DIR = "./fine_tuned_zh_taich"
# ==== 2ï¸âƒ£ è®€å–è³‡æ–™ =================================================
dataset = datasets.load_from_disk(DATA_PATH)
# ==== 3ï¸âƒ£ Tokenizer =================================================
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
tokenizer.model_max_length = 4096
def tokenize(batch):
    # å°‡æºâ†’ç›®æ¨™ä¸²æ¥ï¼Œåšè‡ªå›æ­¸è¨“ç·´
    # å½¢å¼ï¼šinput_ids = src + tgt
    inputs = tokenizer(batch["src"], truncation=True, max_length=2048, padding=False)
    targets = tokenizer(batch["tgt"], truncation=True, max_length=2048, padding=False)
    # ç›´æ¥æŠŠå…©å€‹åºåˆ—æ‹¼èµ·ä¾†
    inputs["labels"] = targets["input_ids"]
    return inputs
dataset = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names["train"])
# ==== 4ï¸âƒ£ æ¨¡å‹ ======================================================
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    torch_dtype=bnb.bnb_4bit,          # 4â€‘bit quantization
    load_in_4bit=True,
    bitsandbytes_4bit_compute_dtype=torch.float16,  # å¦‚æœ GPU æ”¯æŒ float16
    trust_remote_code=True,
)
```
# 4.1 è¨“ç·´æ™‚åªå¾®èª¿ LoRA
```
lora_config = LoraConfig(
    r=16,                      # LoRA hidden size
    lora_alpha=32,
    target_modules=["q_proj","k_proj","v_proj","o_proj"],  # GPTâ€‘é¡è‡ªæ³¨æ„åŠ›å±¤
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, lora_config)
# ==== 5ï¸âƒ£ è¨“ç·´åƒæ•¸ ==============================================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=2,       # æ ¹æ“š GPU RAM èª¿æ•´
    gradient_accumulation_steps=4,      # 4Ã—2 = 8 æ¢
    fp16=True,
    optim="paged_adamw_8bit",           # 8â€‘bit AdamW
    learning_rate=5e-5,
    warmup_steps=200,
    logging_steps=50,
    save_steps=500,
    num_train_epochs=3,
    report_to="none",
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
)
# ==== 6ï¸âƒ£ é–‹å§‹è¨“ç·´ =================================================
trainer.train()
trainer.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
```
<pre>
é—œéµåƒæ•¸
torch_dtype=bnb.bnb_4bit â†’ 4â€‘bit é‡åŒ–ã€‚
gradient_accumulation_steps â†’ ç”¨ä»¥ç´¯ç©æ¢¯åº¦ï¼Œæ¸›å°‘ batch sizeã€‚
lora_r, lora_alpha â†’ LoRA çš„éš±è—å±¤å°ºå¯¸ã€‚
GPU å¿…é ˆæ”¯æ´ float16ï¼ˆå¤§éƒ¨åˆ† RTX 4060/4070/4080/Titan RTX ç­‰ï¼‰ã€‚
6ï¸âƒ£ å¾®èª¿å®Œæˆ â†’ è½‰æˆ .ggufï¼ˆOllama æ ¼å¼ï¼‰
ä¸‹è¼‰ HF è½‰æ›è…³æœ¬ (transformers2gguf)
git clone https://github.com/aspect-build/transformers2gguf.git
cd transformers2gguf
pip install -e .
è½‰æ›
python transformers2gguf.py \
  --model-dir ./fine_tuned_zh_taich \
  --output-dir ./gguf \
  --quantization 4bit --include-model
è½‰æ›å¾Œæœƒå¾—åˆ° fine_tuned_zh_taich.ggufï¼ˆç´„ 6â€“10 GBï¼Œå–æ±ºæ–¼ 4â€‘bit å¤šå°‘å€ï¼‰ã€‚
è‹¥ä½ åªæƒ³åœ¨æœ¬åœ°æ¸¬è©¦ï¼Œå¯ç›´æ¥æŠŠ ./fine_tuned_zh_taich.gguf æ”¾åˆ° ~/.ollama/modelsã€‚
åŠ å…¥ Ollama
ollama add zh_taich ç¿»è­¯-model ./gguf/fine_tuned_zh_taich.gguf
</pre>
<pre>
æ¸¬è©¦
ollama run zh_taich "ä½ å¥½ï¼Œè«‹å•ä½ å¥½å—ï¼Ÿ"
# æœŸæœ›å¾—åˆ° : "ä½ å¥½å’§ï¼Œè«‹å•æå¥½ç„¡ï¼Ÿ"
7ï¸âƒ£ å°æŠ€å·§ & å¸¸è¦‹å•é¡Œ
å•é¡Œ è§£æ±ºæ–¹æ¡ˆ
GPU RAM çˆ†æ‰ 1) å¾ peft æŠŠ lora_r ç”± 16 æ”¹æˆ 8ã€‚
2) æ¸›å°‘ per_device_train_batch_sizeã€‚
3) ç”¨ torch.compile æˆ– accelerate é€²è¡Œ fsdpã€‚
æ¨¡å‹å¤ªæ…¢ ç”¨ 4â€‘bit (bnb_4bit) + fp16ï¼›åŒæ™‚æŠŠ gradient_checkpointing=Trueã€‚
æ²’æœ‰ç¿»è­¯çš„èªæ–™ åœ¨ Hugging Face ä¸Šæœå°‹ zh-tw translationï¼›æˆ–è‡ªå·±ç”¨ huggingface/datasets è£œæˆ train_test_splitã€‚
æƒ³åœ¨æ¨ç†æ™‚åŠ ã€Œå°ˆé–€ç¿»è­¯ã€æç¤º åœ¨ prompt é–‹é ­åŠ ä¸Š translate from Traditional Chinese to Taiwanese...ã€‚
è¨“ç·´æ™‚é–“å¤ªé•· è€ƒæ…®æŠŠæ¨¡å‹åˆ‡æˆ 7B æˆ– 13Bï¼›å°æ–¼ 20Bï¼Œä½ å¯ä»¥æŠŠå¾®èª¿ç›®æ¨™è¨­å®šç‚ºã€ŒæŒ‡ä»¤+ç¿»è­¯ã€åªå¾®èª¿å°‘é‡åƒæ•¸ï¼Œç„¶å¾Œåœ¨ train.py è¨­å®š lora_r=8 å³å¯ã€‚
</pre>
8ï¸âƒ£ ç¤ºç¯„çµæœç¯„ä¾‹
<pre>
å…¥æ¯ å‡ºæ¯
ç¹é«”ä¸­æ–‡: â€œä»Šå¤©å¤©æ°£çœŸå¥½ï¼Œæˆ‘è¦å»æ•£æ­¥ã€‚â€ å°èª: â€œä»Šä»”æ—¥å¤©æ°£çœŸå¥½ï¼Œæˆ‘æ¬²å»èµ°è·¯ã€‚â€
å°èª: â€œæå¥½ç„¡ï¼Œè«‹å•å’±çš„åˆé¤ï¼Ÿâ€ ç¹é«”ä¸­æ–‡: â€œä½ å¥½å—ï¼Œè«‹å•æˆ‘å€‘çš„åˆé¤ï¼Ÿâ€
ä»¥ä¸Šéƒ½æ˜¯åœ¨å¾®èª¿å¾Œæ¨¡å‹ 4â€‘bit ç‰ˆæœ¬çš„å¯¦æ¸¬ã€‚è‹¥ä½ æƒ³æ›´ç²¾ç¢ºï¼ˆå°¤å…¶æ˜¯å°ˆæ¥­è¡“èªï¼‰ï¼Œå†è·‘æ›´å¤šå°ç…§è³‡æ–™å³å¯ã€‚
9ï¸âƒ£ é€²éšï¼šæŠŠå¾®èª¿éçš„æ¨¡å‹éƒ¨ç½²åˆ°æœ¬åœ°æœå‹™
Ollamaï¼šå·²ç¤ºç¯„ ollama add + ollama runã€‚
FastAPI + uvicornï¼šå¦‚æœä½ æƒ³æŠŠæ¨¡å‹æ›æˆ HTTP APIï¼š
</pre>
```
# fastapi_zh_taich.py
from fastapi import FastAPI, Body
import ollama, uvicorn
app = FastAPI()
@app.post("/translate")
async def translate(text: str = Body(...)):
    resp = ollama.run("zh_taich", text)
    return {"translation": resp["response"]}
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```
<pre>
æŠŠé€™å€‹æª”æ”¾é€²åŒä¸€å€‹è™›æ“¬ç’°å¢ƒè£¡ python fastapi_zh_taich.py èµ·ä¾†å³å¯ã€‚
å°çµ
æ­¥é©Ÿ ç›®çš„ é‡è¦æŒ‡ä»¤
1ï¸âƒ£ å®‰è£ç’°å¢ƒ pip install torch transformer datasets peft trl accelerate bitsandbytes
2ï¸âƒ£ æº–å‚™èªæ–™ load_dataset(...) â†’ .save_to_disk()
3ï¸âƒ£ LoRA å¾®èª¿ trainer.train()ï¼ˆä¸Šé¢ç¤ºä¾‹ï¼‰
4ï¸âƒ£ è½‰æ› gguf transformers2gguf.py â†’ ollama add
5ï¸âƒ£ æ¸¬è©¦ ollama run zh_taich "..."
6ï¸âƒ£ éƒ¨ç½² FastAPI æˆ– Streamlit ç›´æ¥å‘¼å« ollama.run
åªè¦ä½ æŠŠé€™å¹¾å€‹æ­¥é©Ÿä¸²èµ·ä¾†ï¼Œå°±èƒ½å¾—åˆ°ä¸€å€‹ã€Œè¯èª â†” å°èªã€å°ˆç”¨çš„ç¿»è­¯æ¨¡å‹ï¼Œä¸¦ä¸”å…¨ç¨‹åœ¨æœ¬æ©Ÿ GPU ä¸Šè·‘ã€‚ç¥ä½ èª¿æ ¡é †åˆ© ğŸš€
</pre>
